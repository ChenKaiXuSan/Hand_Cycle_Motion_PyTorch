<div align="center">

# Pendulum motion Video Classification

<!-- ![CI testing](https://github.com/PyTorchLightning/deep-learning-project-template/workflows/CI%20testing/badge.svg?branch=master&event=push) -->

</div>

## Description  

This is a facility for conducting comparative experiments.
The objective of this experiment is to test the effect of periodic information in motion and symmetric information in action on action recognition networks in periodic motion.
The experimental object of the warehouse is a small hammer undergoing single pendulum motion.

Have a nice code. üòÑ

## Folder structure  

``` bash
.
|-- configs # configuration file
|-- logs # output logs
|-- pendulum_motion # generate dataset code
`-- project 
    |-- dataloader # dataloader for load video
    |-- models # make model 
    |-- trainer # help function for training
    |-- utils
    |-- cross_validation.py # cross validation for prepare datset 
    |-- helper.py # helper function for take results
    `-- main.py # main file for training
```

## How to run

1. install dependencies

``` bash
# clone project   
git clone https://github.com/ChenKaiXuSan/Periodic_Motion_PyTorch.git

# install project   
cd Periodic_Motion_Pytorch/ 
pip install -e .   
pip install -r requirements.txt
```

2. navigate to any file and run it.

```bash
# module folder
cd Periodic_Motion_Pytorch/ 

# run module 
python project/main.py [option] 
```

## Experimental setup

### Dataset

Due to the limitation of the dataset, we decide to use 5 fold cross validation.

> ‚ö†Ô∏è Ensure that the same patient does not appear in training/validation at the same time. 
The number of videos can be guaranteed to be balanced, but the number of patients cannot.

## Data augmentation  

Because the detection method is used to first extract the features of the characters into squares, there is no need to consider the border inconsistency when convolving.
We use both spatial and temporal jittering for augmentation.

Each clip is then generated by randomly cropping windows of size 224x224.
We train and evaluate models with clips of different frames (T = 16, 32, 64) by skipping every other frame (all videos are pre-processed to 30fps).

At the [data_loader.py](project/dataloader/data_loader.py) we defined the transform function.

``` python
self.train_transform = Compose(
            [
                ApplyTransformToKey(
                    key="video",
                    transform=Compose(
                        [
                            UniformTemporalSubsample(self.uniform_temporal_subsample_num),

                            Div255(),
                            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),

                            Resize(size=[self._IMG_SIZE, self._IMG_SIZE]),
                            RandomHorizontalFlip(p=0.5),
                        ]
                    ),
                ),
            ]
        )
```

## Network Structure

So far, we have used the 3D Resnet structure, which are given in the next figure.
![network](imgs/network.png)

## Experimental point

1. different frame with 3DCNN.
2. pre-train/scratch different in experimental.
3. pre-process (object detection extract) make sense in the experimental. (prepare)

## Transfer learning experiments

A natural question that arises is whether these features also generalize to other datasets and class categories.
We examine this question in detail by performing transfer learning experiments on the Kinetis-400. 

‚ö†Ô∏è This time we use the 1s 32 frame and 224x224 video frame shape as the input data.

- fine-tune top layer (head)
- fine-tune last layer (stem) and top layer (head) 
- fine-tune all layers
- train from scratch

## Experimental results

- from scratch
show the Top@1 acc and Top@1 precision respectively.
  
| from scratch | 1s 16frame | 2s 32 frame | 3s 64 frame |
| ------------ | ---------- | ----------- | ----------- |
| accuracy     | 0.36       | 0.61        | 0.68        |
| precision    | 0.69       | 0.75        | 0.70        |

- pre-train on Kinetics-400, which 8 frame length and 8 sample rate.

    sample rate = (num_frames * sample rate) / frame per second

| pretrain  | 1s 16frame | 2s 32 frame | 3s 64 frame |
| --------- | ---------- | ----------- | ----------- |
| accuracy  | 0.65       | 0.62        | **0.81**    |
| precision | 0.88       | 0.71        | nan         |

| ablation study | 1s 8 frame | 2s 8 frame | 3s 8 frame |
| -------------- | ---------- | ---------- | ---------- |
| accuracy       | 0.68       | 0.69       | 0.68       |
| precision      | 0.63       | 0.62       | 0.53       |

| ablation study | 1s 16 frame | 2s 16 frame | 3s 16 frame |
| -------------- | ----------- | ----------- | ----------- |
| accuracy       | x           | 0.71        | 0.67        |
| precision      | x           | 0.57        | 0.59        |

| ablation study | 1s 32 frame | 2s 32 frame | 3s 32 frame |
| -------------- | ----------- | ----------- | ----------- |
| accuracy       | **0.82**    | x           | 0.65        |
| precision      | 0.80        | x           | 0.60        |

x means the number in the first table.  

‚ö†Ô∏è **The accuracy have an exact calculation, but the precision has some problem when calculation, and is not accurate. So we hope the accuracy score will be used for reference.**

## Docker  

We recommend using docker to build the training environment.

1. pull the official docker image, where release in the [pytorchlightning/pytorch_lightning](https://hub.docker.com/r/pytorchlightning/pytorch_lightning)

``` bash  
docker pull pytorchlightning/pytorch_lightning
```

2. create container.

``` bach  
docker run -itd -v $(pwd)/path:/path --gpus all --name container_name --shm-size 32g --ipc="host" <images:latest> bash 

```

3. enter the container and run the code.

``` bash  
docker exec -it container_name bash
```

## About the lib  

stop building wheels üòÑ

### PyTorch Lightning  

[PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. Lightning evolves with you as your projects go from idea to paper/production.

### PyTorch Video  

[link](https://pytorchvideo.org/)
A deep learning library for video understanding research.

### detectron2

[Detectron2](https://detectron2.readthedocs.io/en/latest/index.html) is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of Detectron and maskrcnn-benchmark. It supports a number of computer vision research projects and production applications in Facebook.

### Torch Metrics

[TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/) is a collection of 80+ PyTorch metrics implementations and an easy-to-use API to create custom metrics.

